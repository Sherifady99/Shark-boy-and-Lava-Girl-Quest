{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "dcqu49mWJ2dK",
        "-GfQjHiaKKvB",
        "NCXkOJpxMUD5"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install xlrd\n",
        "!pip install fuzzywuzzy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from fuzzywuzzy import process"
      ],
      "metadata": {
        "id": "nnUrMUTnx-MM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMnbK-ITvpvH"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel(\"/content/GSAF5.xls\")    # Downloading dataset via xls file"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initial examination of dataset"
      ],
      "metadata": {
        "id": "dcqu49mWJ2dK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df   # First impression of the dataset"
      ],
      "metadata": {
        "id": "A5baeBhy0Fco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape    # Viewing the columns & rows of the dataset"
      ],
      "metadata": {
        "id": "qynNtqBK0WHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()   # Gaining information about the count, mean, min, max & standard deviation (only for numerical columns)"
      ],
      "metadata": {
        "id": "cQrTVHaN1D0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.nunique()    # Getting unique values within the dataset"
      ],
      "metadata": {
        "id": "YBS1ZGWQ104V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes # Checking the data types of each column"
      ],
      "metadata": {
        "id": "bxEpQ8XFX7tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum() # Checking for missing values"
      ],
      "metadata": {
        "id": "APlfsNnQ5HXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dealing with column names & dropping irrelevant ones"
      ],
      "metadata": {
        "id": "-GfQjHiaKKvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = df.columns.str.lower().str.replace(' ', '_')   #Changing the column names to be lower case & an \"_\" instead of a space.\n",
        "\n",
        "df.rename(columns = {'species_' : 'species'}, inplace = True) # Fixing species column that had a space at the end"
      ],
      "metadata": {
        "id": "0E3nhmyo2l34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deleted unnecessary columns from the dataset: (We deemed them unnecessary due to our hypothesis or because they were mostly null)\n",
        "df.drop(columns = ['unnamed:_21', 'unnamed:_22', 'year', 'pdf', 'href_formula', 'href', 'case_number', 'case_number.1', 'original_order'], inplace = True)\n"
      ],
      "metadata": {
        "id": "nJlCtRC535A4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "uYdmvcc5wHEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes # Noticed here age is a string so ammending it\n",
        "\n",
        "df.age = pd.to_numeric(df.age, errors = 'coerce') # Converting column to numeric, adding in errors = coerce part to convert non numerical values to nulls and avoid an error.\n",
        "\n",
        "df.dtypes # Confirming it worked"
      ],
      "metadata": {
        "id": "2U8IC4z7wMRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dealing with null values & duplicates"
      ],
      "metadata": {
        "id": "LB1DljmsKQts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Cleaning up some unique values in some columns for improved accuracy\"\"\"\n",
        "\n",
        "df.type = df.type.replace({' Provoked': 'Provoked', 'Questionable': np.nan,'Watercraft': 'Unprovoked', 'Sea Disaster': np.nan, '?': np.nan, 'Unconfirmed': np.nan, 'Unverified': np.nan, 'Invalid': np.nan, 'Boat': 'Unprovoked' })\n",
        "print(df.type.unique()) # Leave 'provoked', 'unprovoked', 'under investigation'\n",
        "\n",
        "\n",
        "df.sex = df.sex.replace({'lli': np.nan, '.': np.nan, 'N': np.nan, 'M ': 'M', ' M': 'M', 'M x 2': 'M'})\n",
        "print(df.sex.unique()) # Leave 'M,' 'F'\n",
        "\n",
        "df['fatal_y/n'] = df['fatal_y/n'].replace({'F': 'Y', 'M': np.nan, 'n': 'N', 'Nq': 'N',\n",
        "                             'UNKNOWN': np.nan, 2017: np.nan, 'Y x 2': 'Y',\n",
        "                             ' N': 'N', 'N ': 'N', 'y': 'Y'}) # Leave 'Y', 'N'\n",
        "df['fatal_y/n'].unique()\n",
        "\n",
        "df.country.unique() # Noticing quite a few countries with a \"?\" at the end so i'll remove them\n",
        "\n",
        "df.country = df.country.str.rstrip(\"?\") # Using strip to remove those with a ? and keep the rest.\n",
        "\n",
        "df.species = df.species.str.lower()\n",
        "df.species = df.species.str.strip()\n",
        "  # remove all capital letters and trailing spaces\n",
        "\n",
        "df.species.nunique()"
      ],
      "metadata": {
        "id": "QicrnqRQ3cHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(subset=['fatal_y/n', 'type'], how='all') # Dropping rows with fatal & type that are null as they don't add value to Hypothesis 1\n",
        "df = df.dropna(subset=['fatal_y/n', 'species'], how='all')  # Dropping rows with fatal & species that are null as they don't add value to Hypothesis 2\n",
        "df = df.dropna(subset=['name', 'sex'], how='all') # Dropping 70 rows that had both name & sex null so we can fill them in later on."
      ],
      "metadata": {
        "id": "FKUqhNEXwW_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using regex to fix data column format:\n",
        "\n",
        "df[\"date\"] = df[\"date\"].apply(lambda x: re.findall(r\"\\b(?:\\d{4}-\\d{2}-\\d{2}|\\d{2}-[A-Za-z]{3}-\\d{4})\\b\", str(x))[0] if len(re.findall(r\"\\b(?:\\d{4}-\\d{2}-\\d{2}|\\d{2}-[A-Za-z]{3}-\\d{4})\\b\", str(x))) > 0 else np.NaN)\n",
        "\n",
        "df.date.isna().sum()  # Now created 853 nulls due to adjusting the format\n",
        "\n",
        "df.date.fillna('Unverified', inplace = True)  # Filling nulls with unverified as we don't want to lose 853 rows in our data."
      ],
      "metadata": {
        "id": "o-cQaMbiwXCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filling in null names with John or Jane Doe, as a better way to display the data than marking unknown.\n",
        "\n",
        "df.loc[(df.sex == \"M\") & (df.name.isna()), 'name'] = 'John Doe'\n",
        "df.loc[(df.sex == \"F\") & (df.name.isna()), 'name'] = 'Jane Doe'\n",
        "\n",
        "df.loc[df.name.isin(['John Doe', 'Jane Doe'])]  # Checking that it worked."
      ],
      "metadata": {
        "id": "vsUjbojSwXFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Filling in remaining nulls in the columns. Some we replaced with just unknowns as its not very relevant to our hypothesis and others we used aggregation to fill.\"\"\"\n",
        "\n",
        "df.dropna(subset = ['type'], inplace = True) # Dropping all rows with type as null to not mess with Hypothesis 1\n",
        "df.country.fillna('Unknown', inplace = True)   # Only 50 null countries so marking unknown.\n",
        "df.state.fillna('Not Confirmed', inplace = True)  # Marking the state nulls as not confirmed as they are also irrelevant to our hypothesis\n",
        "df.location.fillna('Unknown', inplace = True) # Marking location nulls as unknown to not contaminate dataset.\n",
        "df.activity.fillna('Unknown', inplace = True) # Marking missing activities as unknown.\n",
        "df.sex = df.sex.fillna(df.sex.mode()[0])  # Filling in the null sex values with the mode, shouldn't impact our hypothesis at all.\n",
        "df.age = df.age.fillna(df.age.mean()) # Filling in the null age values with the mean, to not affect the data too much or affect our hypothesis.\n",
        "df.injury.fillna('Not Specified', inplace = True) # Filling in only a handfull of null injuries with not specified\n",
        "df.dropna(subset = ['fatal_y/n'], inplace = True) # Dropping only 29 rows with fatality as null because it is very important for both of our hypothesis.\n",
        "df.time.fillna('Not Specified', inplace = True)   # Filling in the time null values with unknowns as there are over 2807 null rows and we can't lose that much of our data.\n",
        "df.source.fillna('Unconfirmed', inplace = True) # Filling in the nulls of source with unconfirmed as it is not very relevant to our hypothesis."
      ],
      "metadata": {
        "id": "hWLSlx6AwXHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum() # We will clean species once we start hypothesis 2, we don't want to lose 2776 rows for hypothesis 1"
      ],
      "metadata": {
        "id": "4VwuiYtZwXKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum() # Found there are 2 duplicate rows\n",
        "df.drop_duplicates(inplace = True)  # Dropping both rows as they are irrelevant\n",
        "df.duplicated().sum() # Checking its 0"
      ],
      "metadata": {
        "id": "O8bWDEEZgcBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothesis 1: Provoked shark attacks are more fatal than unprovoked attacks."
      ],
      "metadata": {
        "id": "_F-ZxdMWJaV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hypothesis 1: Provoked shark attacks are more deadly than unprovoked\n",
        "\n",
        "df_hypothesis_1 = df[['name','sex','type','fatal_y/n']] # Created new data frame specific to hypothesis 1 with only relevant columns for us."
      ],
      "metadata": {
        "id": "d3r3xiev5EDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for remaining null values in type & fatal\n",
        "\n",
        "print(\"Remaining Null Values in 'type' column:\", df_hypothesis_1['type'].isna().sum())\n",
        "print(\"Remaining Null Values in 'fatal_y/n' column:\", df_hypothesis_1['fatal_y/n'].isna().sum())"
      ],
      "metadata": {
        "id": "tiPkyU7p_rfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hypothesis_1 = df_hypothesis_1.reset_index(drop=True)    # Resetting index after dropping values.\n",
        "\n",
        "df_hypothesis_1"
      ],
      "metadata": {
        "id": "5bXmPkM_9_Nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_hypothesis_1 = df_hypothesis_1.groupby(['type', 'fatal_y/n']).size().unstack(fill_value=0)  # Grouping by type & fatal result to see our results.\n",
        "\n",
        "grouped_hypothesis_1['fatality_rate'] = round(grouped_hypothesis_1['Y'] / (grouped_hypothesis_1['Y'] + grouped_hypothesis_1['N']) * 100, 2).astype(str) + '%'   # Adding in a column for fatality rate to test our theory.\n",
        "\n",
        "grouped_hypothesis_1  # Seeing final result --> Hypothesis 1 is wrong in the end."
      ],
      "metadata": {
        "id": "1hQM72zmBheX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Visualising data using a bar chart\"\"\"\n",
        "\n",
        "# Create a new DataFrame to visualise by copying grouped_hypothesis_1 to make some changes to it without altering original.\n",
        "visual_hypothesis_1 = grouped_hypothesis_1.copy()\n",
        "\n",
        "# Changing the fatality rate to a float and removing % sign to use for bar chart\n",
        "visual_hypothesis_1['fatality_rate'] = visual_hypothesis_1['fatality_rate'].str.rstrip(\"%\").astype(float)\n",
        "\n",
        "# Excluding 'Under investigation' type to make the data more clean:\n",
        "visual_hypothesis_1 = visual_hypothesis_1.drop(index='Under investigation')\n",
        "\n",
        "# Calculating the inverse (non-fatal percentage) in order to make it a stacked bar chart.\n",
        "visual_hypothesis_1['non_fatal_rate'] = 100 - visual_hypothesis_1['fatality_rate']\n",
        "\n",
        "# Plotting stacked bar chart with fatality rate as the y axis & attack type is the x axis.\n",
        "visual_hypothesis_1[['fatality_rate', 'non_fatal_rate']].plot(kind='bar', stacked=True, figsize=(9, 5), title=\"Fatality Rate by Attack Type\", ylabel=\"Fatality Rate (%)\", xlabel=\"Attack Type\", color=['red', 'blue'])"
      ],
      "metadata": {
        "id": "WPhkf9eVMbCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothesis 2: Great whites are the most fatal"
      ],
      "metadata": {
        "id": "NCXkOJpxMUD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hypothesis 2: Great white are the deadliest shark species in terms of total fatalities\n",
        "\n",
        "# Check out the species column. We see lots of unique values\n",
        "print(df.species.value_counts())\n",
        "print(df.species.nunique())"
      ],
      "metadata": {
        "id": "jW3YbM7ZMVNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a baby dataframe to compare species and fatal_y/n\n",
        "\n",
        "df2 = df[['species', 'fatal_y/n']]\n",
        "df2"
      ],
      "metadata": {
        "id": "62EieRd6xDtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df2.species.isna().sum()) # Check for nulls\n",
        "df2 = df2.dropna() # Drop nulls\n",
        "df2"
      ],
      "metadata": {
        "id": "bJjU0BWDxDvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove 'not stated' from species\n",
        "\n",
        "df2 = df2[df2['species'] != 'not stated']\n",
        "df2.reset_index(drop=True, inplace=True) # Reset index\n",
        "df2 # 6 rows removed"
      ],
      "metadata": {
        "id": "PA1CyrOPxDzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To further clean up the column, we will use fuzzy matching and exact matching.\n",
        "\n",
        "# First, create a list of valid species names.\n",
        "\n",
        "# Found a list of 35 species known to have been involved in shark attacks online\n",
        "# (https://www.floridamuseum.ufl.edu/shark-attacks/factors/species-implicated/).\n",
        "\n",
        "valid_species = ['grey reef', 'bronze whaler', 'spinner', 'silky', 'galapagos', 'bull', 'blacktip',\n",
        "'oceanic whitetip', 'blacktip reef', 'dusky', 'caribbean reef', 'sandbar', 'requiem', 'sand tiger',\n",
        "'white', 'tiger', 'tope', 'nurse', 'port jackson', 'cookiecutter', 'shortfin mako', 'mako', 'porbeagle',\n",
        "'lemon', 'sevengill', 'spotted wobbegong', 'ornate wobbegong', 'wobbegong', 'blue', 'guitarfish',\n",
        "'hammerhead', 'whitetip reef', 'atlantic angel', 'leopard'] # Removed 'shark' to help with fuzzy matching (everything would match with 'shark')\n",
        "\n",
        "# Then, we fuzzy match\n",
        "\n",
        "# Create fuzzy matching function\n",
        "def fuzzy_match_for_species(species_name):\n",
        "    if species_name is not None:  # If the species name is not None (not NaN)\n",
        "        best_match, score = process.extractOne(species_name, valid_species) # finds the value from valid_species most similar to the given string\n",
        "        if score >= 80:  # Fuzzy match with threshold of 80\n",
        "            return best_match\n",
        "    return species_name  # Return the original species name if no match\n",
        "\n",
        "# Apply fuzzy matching to species column and add fuzzy match column\n",
        "df2['fuzzy_match'] = df2['species'].apply(fuzzy_match_for_species)\n",
        "\n",
        "df2"
      ],
      "metadata": {
        "id": "FFdCNe7lxD3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.fuzzy_match.nunique() # Still plenty of unique values, but no longer 1500"
      ],
      "metadata": {
        "id": "cVXjNEZcxD5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, find exact matches in the fuzzy_match column\n",
        "\n",
        "# Create function\n",
        "\n",
        "def check_exact_match(species):\n",
        "    for valid_species_name in valid_species:\n",
        "        if valid_species_name in species:  # Check if the valid species name is in the string\n",
        "            return valid_species_name\n",
        "    return None  # Return None if no exact match found\n",
        "\n",
        "# Step 4: Apply exact match check to fuzzy matches\n",
        "df2['match'] = df2['fuzzy_match'].apply(check_exact_match)\n",
        "\n",
        "df2\n",
        "\n"
      ],
      "metadata": {
        "id": "2wcHT8GlxD7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.match.isna().sum() # No match found for 1230 values"
      ],
      "metadata": {
        "id": "Re2FG-qLxD95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df2.dropna() # drop the nulls\n",
        "df2 = df2.reset_index(drop=True)  # Reset the index\n",
        "df2"
      ],
      "metadata": {
        "id": "o8aOJw0IxEAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now to figure out which species have the most fatalities!\n",
        "\n",
        "fatalities_per_species = df2.groupby('match')['fatal_y/n'].apply(lambda x: (x == 'Y').sum())\n",
        "fatalities_per_species = fatalities_per_species[fatalities_per_species > 0] # exclude non-fatal species\n",
        "# Sort the species by the count of 'Y' (fatalities), from high to low\n",
        "fatalities_per_species = fatalities_per_species.sort_values(ascending=False)\n",
        "fatalities_per_species"
      ],
      "metadata": {
        "id": "q7TIIsxyxpgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bar chart for the 11 fatal species\n",
        "print(type(fatalities_per_species))\n",
        "# fatalities_per_species = fatalities_per_species[fatalities_per_species['fatal_y/n']!= 0]\n",
        "\n",
        "fatalities_per_species.plot(kind='barh', x='match', y='fatal_y/n', title='Fatal attacks per species', figsize=(10, 6), color='skyblue')\n",
        "\n"
      ],
      "metadata": {
        "id": "sq0Gth6txpi5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}